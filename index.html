<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Xudong Cai</title>
  <meta name="author" content="Xudong Cai">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" href="images/favicon.svg" type="image/svg+xml">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:0px">
          <!-- Header section: name, bio and links -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align:center; font-size: 36px; font-weight: bold; margin: 0 0 8px 0;">
                    Xudong Cai</p>
                  <p>
                    I am a Ph.D. candidate at <strong>Renmin University of China (RUC)</strong>, advised by
                    <a href="https://yongcaiwang.github.io/index_en.html"><strong>Yongcai Wang</strong></a> and
                    <a href="http://info.ruc.edu.cn/jsky/szdw/ajxjgcx/jsjkxyjsx1/js2/c2523870862c49758d02a6705c1e1556.htm"><strong>Deying Li</strong></a>,
                    and a visiting student in the <a href="https://physicalvision.github.io/people"><strong>Physical Vision Group (PVG)</strong></a>
                    at <strong>Nanyang Technological University (NTU)</strong>, working with
                    <a href="https://physicalvision.github.io/people"><strong>Chuanxia Zheng</strong></a>. My research
                    focuses on <strong>3D computer vision</strong>, with an emphasis on <strong>3D scene reconstruction
                      and generation</strong>; I welcome discussions and collaborations—feel free to reach out.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:xudongcai.ruc@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=TkwComsAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/Master-cai/">GitHub</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%;vertical-align:middle">
                  <a href="images/avatar.jpg"><img style="width:100%;max-width:100%;object-fit:cover;border-radius:50%;"
                      src="images/avatar.jpg" alt="Profile photo" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Research summary -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    I mainly work on <strong>3D computer vision</strong>, with an emphasis on <strong>3D scene
                      reconstruction and generation</strong>.
                  </p>
                  <ul>
                    <li>Scene Generation</li>
                    <li>3D / 4D scene reconstruction</li>
                    <li>Image matching</li>
                    <li>Visual Place Relocalization</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

	          <!-- Publications section header -->
	          <h2>Publications</h2>
	          <p>The papers are listed in the following order.</p>

          <!-- Publications list in table format -->
          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- 2026: Mem4D (AAAI) -->
              <tr bgcolor="#ffffd0">
                <td style="padding:16px;width:30%;vertical-align:middle">
                  <img src="images/Mem4D.png" width="100%" alt="Paper thumbnail">
                </td>
                <td style="padding:8px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2508.07908"><span class="papertitle">Mem4D: Decoupling Static and
                      Dynamic
                      Memory for Dynamic Scene Reconstruction</span></a>
                  <br>
                  <strong>Xudong&nbsp;Cai</strong>, Shuo&nbsp;Wang, Peng&nbsp;Wang, Yongcai&nbsp;Wang, Zhaoxin&nbsp;Fan,
                  Wanting&nbsp;Li, Tianbao&nbsp;Zhang, Jianrong&nbsp;Tao, Yeying&nbsp;Jin, Deying&nbsp;Li
                  <br>
                  <em>AAAI</em>, 2026
                  <br>
                  <a href="https://arxiv.org/abs/2508.07908">paper</a> /
                  <a href="https://master-cai.github.io/">Project</a> /
                  <a href="https://master-cai.github.io/">Code</a>
                  <p></p>
                  <p>
                    Mem4D decouples static geometry and dynamic motion by introducing persistent structure memory and
                    transient dynamics memory, enabling high‑fidelity dynamic scene reconstruction with global
                    consistency.
                  </p>
                </td>
              </tr>

	              <!-- 2026: MonoDream (AAAI) -->
	              <tr>
                <td style="padding:16px;width:30%;vertical-align:middle">
                  <img src="images/MonoDream.png" width="100%" alt="Paper thumbnail">
                </td>
                <td style="padding:8px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2508.02549"><span class="papertitle">MonoDream: Monocular Vision‑Language Navigation with Panoramic
                      Dreaming</span></a>
                  <br>
                  Shuo&nbsp;Wang, Yongcai&nbsp;Wang, Zhaoxin&nbsp;Fan, Yucheng&nbsp;Wang, Maiyue&nbsp;Chen,
                  Kaihui&nbsp;Wang,
                  Zhizhong&nbsp;Su, Wanting&nbsp;Li, <strong>Xudong&nbsp;Cai</strong>, Yeying&nbsp;Jin, Deying&nbsp;Li
                  <br>
                  <em>AAAI</em>, 2026
                  <br>
                  <a href="https://arxiv.org/abs/2508.02549">paper</a> /
                  <a href="https://master-cai.github.io/">Project</a> /
                  <a href="https://master-cai.github.io/">Code</a>
                  <p></p>
                  <p>
                    We propose a monocular vision‑language navigation framework that "dreams" panoramic context from
                    partial observations to bridge the gap between limited field‑of‑view inputs and global spatial
                    reasoning.
                  </p>
                </td>
	              </tr>
	
	              <!-- 2025: Aux‑Think (NeurIPS) -->
	              <tr>
	                <td style="padding:16px;width:30%;vertical-align:middle">
	                  <img src="images/Aux-Think.png" width="100%" alt="Paper thumbnail">
                </td>
                <td style="padding:8px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2505.11886"><span class="papertitle">Aux‑Think: Exploring Reasoning Strategies for Data‑Efficient
                      Vision‑Language Navigation</span></a>
                  <br>
                  Shuo&nbsp;Wang, Yongcai&nbsp;Wang, Wanting&nbsp;Li, <strong>Xudong&nbsp;Cai</strong>,
                  Yucheng&nbsp;Wang,
                  Maiyue&nbsp;Chen, Kaihui&nbsp;Wang, Zhizhong&nbsp;Su, Deying&nbsp;Li, Zhaoxin&nbsp;Fan
                  <br>
                  <em>NeurIPS</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/2505.11886">paper</a> /
                  <a href="https://master-cai.github.io/">Project</a> /
                  <a href="https://master-cai.github.io/">Code</a>
                  <p></p>
                  <p>
                    This work explores auxiliary reasoning tasks to improve data efficiency in vision‑language
                    navigation,
                    allowing agents to learn complex navigational strategies without massive datasets.
	                  </p>
	                </td>
	              </tr>
	
	              <!-- 2024: Dust to Tower (arXiv) -->
	              <tr bgcolor="#ffffd0">
	                <td style="padding:16px;width:30%;vertical-align:middle">
	                  <img src="images/Dust2Tower.png" width="100%" alt="Paper thumbnail">
	                </td>
	                <td style="padding:8px;width:70%;vertical-align:middle">
	                  <a href="https://arxiv.org/abs/2412.19518"><span class="papertitle">Dust to Tower: Coarse‑to‑Fine
	                      Photo‑Realistic Scene Reconstruction from Sparse Uncalibrated Images</span></a>
	                  <br>
	                  <strong>Xudong&nbsp;Cai</strong>, Yongcai&nbsp;Wang, Zhaoxin&nbsp;Fan, Haoran&nbsp;Deng,
	                  Shuo&nbsp;Wang,
	                  Wanting&nbsp;Li, Deying&nbsp;Li, Lun&nbsp;Luo, Minhang&nbsp;Wang, Jintao&nbsp;Xu
	                  <br>
	                  <em>arXiv</em>, 2024
	                  <br>
	                  <a href="https://arxiv.org/abs/2412.19518">paper</a> /
	                  <a href="https://master-cai.github.io/">Project</a> /
	                  <a href="https://master-cai.github.io/">Code</a>
	                  <p></p>
	                  <p>
	                    Dust&nbsp;to&nbsp;Tower introduces an accurate and efficient coarse‑to‑fine framework that jointly
	                    optimizes 3D Gaussian Splatting and image poses from sparse, uncalibrated images, enabling
	                    photo‑realistic scene reconstruction.
	                  </p>
	                </td>
	              </tr>
	
	              <!-- 2025: MambaVO (CVPR) -->
	              <tr>
	                <td style="padding:16px;width:30%;vertical-align:middle">
	                  <img src="images/MambaVO.png" width="100%" alt="Paper thumbnail">
	                </td>
	                <td style="padding:8px;width:70%;vertical-align:middle">
	                  <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MambaVO_Deep_Visual_Odometry_Based_on_Sequential_Matching_Refinement_and_CVPR_2025_paper.html"><span class="papertitle">MambaVO: Deep Visual Odometry Based on Sequential Matching
	                      Refinement and Training Smoothing</span></a>
	                  <br>
	                  Shuo&nbsp;Wang, Wanting&nbsp;Li, Yongcai&nbsp;Wang, Zhaoxin&nbsp;Fan, Zongxiang&nbsp;Huang,
	                  <strong>Xudong&nbsp;Cai</strong>, Jun&nbsp;Zhao, Deying&nbsp;Li
	                  <br>
	                  <em>CVPR</em>, 2025
	                  <br>
	                  <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MambaVO_Deep_Visual_Odometry_Based_on_Sequential_Matching_Refinement_and_CVPR_2025_paper.html">paper</a> /
	                  <a href="https://master-cai.github.io/">Project</a> /
	                  <a href="https://master-cai.github.io/">Code</a>
	                  <p></p>
	                  <p>
	                    MambaVO integrates sequential matching refinement and a trending‑aware training penalty into a
	                    Mamba‑based architecture for efficient visual odometry, yielding robust pose estimation and mapping.
	                  </p>
	                </td>
	              </tr>
	
	              <!-- 2024: PRISM (ACM MM) -->
	              <tr bgcolor="#ffffd0">
	                <td style="padding:16px;width:30%;vertical-align:middle">
	                  <img src="images/PRISM.png" width="100%" alt="Paper thumbnail">
                </td>
                <td style="padding:8px;width:70%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/10.1145/3664647.3681209"><span class="papertitle">PRISM: PRogressive
                      dependency maxImization for Scale‑invariant image Matching</span></a>
                  <br>
                  <strong>Xudong&nbsp;Cai</strong>, Yongcai&nbsp;Wang, Lun&nbsp;Luo, Minhang&nbsp;Wang, Deying&nbsp;Li,
                  Jintao&nbsp;Xu, Wenting&nbsp;Gu, Rui&nbsp;Ai
                  <br>
                  <em>ACM&nbsp;Multimedia</em>, 2024
                  <br>
                  <a href="https://dl.acm.org/doi/10.1145/3664647.3681209">paper</a> /
                  <a href="https://master-cai.github.io/">Project</a> /
                  <a href="https://master-cai.github.io/">Code</a>
                  <p></p>
                  <p>
                    PRISM maximizes progressive dependency to explicitly model scale relationships, improving feature
                    matching accuracy for images with extreme scale variations.
                  </p>
                </td>
              </tr>

              <!-- 2024: Voloc (ICRA) -->
              <tr bgcolor="#ffffd0">
                <td style="padding:16px;width:30%;vertical-align:middle">
                  <img src="images/VOLoc.png" width="100%" alt="Paper thumbnail">
                </td>
                <td style="padding:8px;width:70%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/document/10610530"><span class="papertitle">Voloc: Visual Place
                      Recognition by Querying Compressed LiDAR Map</span></a>
                  <br>
                  <strong>Xudong&nbsp;Cai</strong>, Yongcai&nbsp;Wang, Zongxiang&nbsp;Huang, Yiqing&nbsp;Shao,
                  Deying&nbsp;Li
                  <br>
                  <em>ICRA</em>, 2024
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/10610530">paper</a> /
                  <a href="https://master-cai.github.io/">Project</a> /
                  <a href="https://master-cai.github.io/">Code</a>
                  <p></p>
                  <p>
                    Voloc performs cross‑modal visual place recognition by querying a compressed LiDAR map with visual
                    images, achieving robust localization in large‑scale environments with reduced storage.
                  </p>
                </td>
              </tr>

              <!-- 2024: Vsformer (TVCG) -->
              <tr>
                <td style="padding:16px;width:30%;vertical-align:middle">
                  <img src="images/VSFormer.png" width="100%" alt="Paper thumbnail">
                </td>
                <td style="padding:8px;width:70%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/document/10478769"><span class="papertitle">Vsformer: Mining
                      Correlations in Flexible View Set for Multi‑view 3D Shape Understanding</span></a>
                  <br>
                  Hao&nbsp;Sun, Yongcai&nbsp;Wang, Peng&nbsp;Wang, Haoran&nbsp;Deng, <strong>Xudong&nbsp;Cai</strong>,
                  Deying&nbsp;Li
                  <br>
                  <em>IEEE&nbsp;TVCG</em>, 2024
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/10478769">paper</a> /
                  <a href="https://master-cai.github.io/">Project</a> /
                  <a href="https://master-cai.github.io/">Code</a>
                  <p></p>
                  <p>
                    Vsformer is a transformer‑based architecture that mines correlations within a flexible set of views
                    to
                    enhance multi‑view 3D shape classification and retrieval.
                  </p>
                </td>
	              </tr>
	
	              <!-- 2023: ColSLAM (ACM MM) -->
	              <tr>
	                <td style="padding:16px;width:30%;vertical-align:middle">
	                  <img src="images/ColSLAM.png" width="100%" alt="Paper thumbnail">
                </td>
                <td style="padding:8px;width:70%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/10.1145/3581783.3611995"><span class="papertitle">ColSLAM: A
                      Versatile Collaborative SLAM System for Mobile Phones Using Point‑Line Features and Map
                      Caching</span></a>
                  <br>
                  Wanting&nbsp;Li, Yongcai&nbsp;Wang, Yongyu&nbsp;Guo, Shuo&nbsp;Wang, Yu&nbsp;Shao, Xuewei&nbsp;Bai,
                  <strong>Xudong&nbsp;Cai</strong>, Qiang&nbsp;Ye, Deying&nbsp;Li
                  <br>
                  <em>ACM&nbsp;Multimedia</em>, 2023
                  <br>
                  <a href="https://dl.acm.org/doi/10.1145/3581783.3611995">paper</a> /
                  <a href="https://master-cai.github.io/">Project</a> /
                  <a href="https://master-cai.github.io/">Code</a>
                  <p></p>
                  <p>
                    ColSLAM is a collaborative SLAM system designed for mobile phones that fuses point and line features
                    and introduces a novel map caching mechanism to support robust multi‑agent mapping.
	                  </p>
	                </td>
	              </tr>
	
	              <!-- 2023: 3D Object Detection (Advanced Engineering Informatics) -->
	              <tr>
                <td style="padding:16px;width:30%;vertical-align:middle">
                  <img src="images/AEI.png" width="100%" alt="Paper thumbnail">
                </td>
                <td style="padding:8px;width:70%;vertical-align:middle">
                  <a href="https://doi.org/10.1016/j.aei.2023.101971"><span class="papertitle">An Object Detection
                      Algorithm Combining Semantic and Geometric Information of the 3D Point Cloud</span></a>
                  <br>
                  Zongxiang&nbsp;Huang, Yongcai&nbsp;Wang, Jin&nbsp;Wen, Peng&nbsp;Wang,
                  <strong>Xudong&nbsp;Cai</strong>
                  <br>
                  <em>Advanced Engineering Informatics</em>, 2023
                  <br>
                  <a href="https://doi.org/10.1016/j.aei.2023.101971">paper</a> /
                  <a href="https://master-cai.github.io/">Project</a> /
                  <a href="https://master-cai.github.io/">Code</a>
                  <p></p>
                  <p>
                    This work fuses semantic features and geometric structures from point clouds to improve 3D object
                    detection accuracy in complex environments.
	                  </p>
	                </td>
	              </tr>
	
	              <!-- 2024: Survey on Visual Relocalization -->
	              <tr bgcolor="#ffffd0">
	                <td style="padding:16px;width:30%;vertical-align:middle">
	                  <img src="images/Survey.png" width="100%" alt="Paper thumbnail">
	                </td>
	                <td style="padding:8px;width:70%;vertical-align:middle">
	                  <a href="https://www.jos.org.cn/jos/article/abstract/6946"><span
	                      class="papertitle">Survey on Visual Relocalization in Prior Map (基于先验地图的视觉重定位方法综述)</span></a>
	                  <br>
	                  <strong>Xudong&nbsp;Cai</strong>, Yongcai&nbsp;Wang, Xuewei&nbsp;Bai, Deying&nbsp;Li
	                  <br>
	                  <em>Journal of Software</em>, 2024
	                  <br>
	                  <a href="https://www.jos.org.cn/jos/article/abstract/6946">paper</a> /
	                  <a href="https://master-cai.github.io/">Project</a> /
	                  <a href="https://master-cai.github.io/">Code</a>
	                  <p></p>
	                  <p>
	                    This Chinese‑language survey categorizes visual relocalization methods based on prior maps (2D,
	                    point‑cloud, etc.) and discusses current challenges and future directions.
	                  </p>
	                </td>
	              </tr>

            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>
